{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Twin evaluation example\nThis example shows how you can use pyTwin\nto load and evaluate a Twin model. The model consists in a coupled clutches\nwith 4 inputs (applied torque, 3 clutches opening) and 3 outputs (computed\ntorque on each of the clutches)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# sphinx_gallery_thumbnail_path = '_static/coupledClutches.png'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Perform required imports\nPerform required imports, which includes downloading and importing the\ninput files\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import platform\nimport os\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom pytwin.evaluate import TwinModel\nfrom pytwin import examples\n\ntwin_file = examples.download_file(\"CoupledClutches_23R1_other.twin\",\n                                   \"twin_files\")\ncsv_input = examples.download_file(\"CoupledClutches_input.csv\",\n                                   \"twin_input_files\")\ntwin_config = examples.download_file(\"CoupledClutches_config.json\",\n                                     \"twin_input_files\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Auxiliary functions definition\nPost processing for results comparison.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_result_comparison(step_by_step_results: pd.DataFrame,\n                           batch_results: pd.DataFrame):\n    \"\"\"Compare the results obtained from 2 different simulations executed\n    on the same TwinModel. The 2 results dataset are provided as Pandas\n    Dataframe. The function will plot the different results for all the\n    outputs and save the plot as a file \"results.png\" \"\"\"\n    pd.set_option('display.precision', 12)\n    pd.set_option('display.max_columns', 20)\n    pd.set_option('display.expand_frame_repr', False)\n\n    # Plotting the runtime outputs\n    columns = step_by_step_results.columns[1::]\n    result_sets = 2  # Results from only step-by-step, batch_mode\n    fig, ax = plt.subplots(ncols=result_sets, nrows=len(columns),\n                           figsize=(18, 7))\n    if len(columns) == 1:\n        single_column = True\n    else:\n        single_column = False\n\n    fig.subplots_adjust(hspace=0.5)\n    fig.set_tight_layout({\"pad\": .0})\n\n    for ind, col_name in enumerate(columns):\n        # Plot runtime results\n        if single_column:\n            axes0 = ax[0]\n            axes1 = ax[1]\n\n        else:\n            axes0 = ax[ind, 0]\n            axes1 = ax[ind, 1]\n\n        step_by_step_results.plot(x=0, y=col_name, ax=axes0, ls=\":\", color='g',\n                                  title='Twin Runtime - Step by Step')\n        axes0.legend(loc=2)\n        axes0.set_xlabel('Time [s]')\n\n        # Plot Twin batch mode csv results\n        batch_results.plot(x=0, y=col_name, ax=axes1, ls=\"-.\", color='g',\n                           title='Twin Runtime - Batch Mode')\n        axes1.legend(loc=2)\n        axes1.set_xlabel('Time [s]')\n\n        if ind > 0:\n            axes0.set_title('')\n            axes1.set_title('')\n\n    # Show plot\n    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Defining external files path\nDefining the runtime log path as well as loading the input data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#runtime_log = os.path.join(cur_dir, 'model_{}.log'.format(platform.system())) # TODO remove/refactor based on new logging file\ntwin_model_input_df = examples.load_data(csv_input)\ndata_dimensions = twin_model_input_df.shape\nnumber_of_datapoints = data_dimensions[0] - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading the Twin Runtime and instantiating it\nLoading the Twin Runtime and instantiating it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('Loading model: {}'.format(twin_file))\ntwin_model = TwinModel(twin_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting up the initial settings of the Twin and initializing it\nDefining the initial inputs of the Twin, initializing it and collecting\nthe initial outputs values\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "twin_model.initialize_evaluation(json_config_filepath=twin_config)\noutputs = [twin_model.evaluation_time]\nfor item in twin_model.outputs:\n    outputs.append(twin_model.outputs[item])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step by step simulation mode\nLooping over all the input data, simulating the Twin one time step at a\ntime and collecting corresponding outputs\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sim_output_list_step = [outputs]\ndata_index = 0\nwhile data_index < number_of_datapoints:\n    # Gets the stop time of the current simulation step\n    time_end = twin_model_input_df.iloc[data_index + 1][0]\n    step = time_end - twin_model.evaluation_time\n    inputs = dict()\n    for column in twin_model_input_df.columns[1::]:\n        inputs[column] = twin_model_input_df[column][data_index]\n    twin_model.evaluate_step_by_step(step_size=step, inputs=inputs)\n    outputs = [twin_model.evaluation_time]\n    for item in twin_model.outputs:\n        outputs.append(twin_model.outputs[item])\n    sim_output_list_step.append(outputs)\n    data_index += 1\nresults_step_pd = pd.DataFrame(sim_output_list_step,\n                               columns=['Time'] + list(twin_model.outputs),\n                               dtype=float)\n\n# ##############################################################################\n# Batch simulation mode\n# ~~~~~~~~~~~~~~~~~~~~~\n# Resetting/re-initializing the Twin and running it in batch mode (i.e. passing\n# all the input data, simulating all the data points, and collecting all\n# the outputs at once)\n\n\ndata_index = 0\ninputs = dict()\nfor column in twin_model_input_df.columns[1::]:\n    inputs[column] = twin_model_input_df[column][data_index]\ntwin_model.initialize_evaluation(inputs=inputs,\n                                 json_config_filepath=twin_config)\noutputs = [twin_model.evaluation_time]\nfor item in twin_model.outputs:\n    outputs.append(twin_model.outputs[item])\nresults_batch_pd = twin_model.evaluate_batch(twin_model_input_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Post processing\nPlotting the different results and saving the image on disk\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_result_comparison(results_step_pd, results_batch_pd)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}